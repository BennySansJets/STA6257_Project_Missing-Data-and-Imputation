---
title: "Missing Data and Imputation"
author: "Natalie Belford, Benjamin Brustad, Jasmine Hyler"
date: '`r Sys.Date()`'
format: revealjs
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references

#always_allow_html: true # this allows to get PDF with HTML features

self-contained: true
execute: 
  warning: false
  message: false 
---

## Introduction

- Missing Data

  - Common occurrence among datasets
    - Not widely discussed how to resolve 
  - Alone, not a problem
    - Becomes problem when analysis shows bias or lacks power
    
## Issues with Missing Data

- Within experiments and analyses, missing data can lead to

  - Inaccurately distributed data and calculations 
  - Skewed visuals (histograms, line graphs, etc.)
  - Inaccurate conclusions rendered


## Types of Missing Data 
- Missing data categorized into one of three types:
  - Missing Completely at Random (MCAR)
  - Missing at Random (MAR)
  - Missing Not at Random (MNAR)
 
 
## Missing Completely at Random (MCAR)
- Probability that missing data either 
  - Not related to specific value supposed to be obtained
  - Related to set of obtained values
  - MCAR data is unbiased
  
  
## Missing Completely at Random (MCAR), continued
- Examples:
  - Equipment failure
  - Samples lost in transit 
  - Unsatisfactory samples

  

## Missing at Random (MAR)
- Probability that missing responses are 
  - Dependent upon set of observed responses
  - Not related to specific expected values 
- Most realistic option for missing data
- Missing data is knowable, missingness is predictable
  - Missingness not at random; is random condition on observed values from entire dataset  
  - Estimates determined
  - Bias can be "recovered"
  
  
## Missing at Random (MAR), continued
- Example: Men less likely to fill out depression survey 
    - Reason: because of society
       - Not because of lack of depression symptoms 


## Missing Not at Random (MNAR)
- Missing data not classified as either MCAR or MAR
  - No bias is observed
  - Power is affected
    - Larger Standard Error (SE) due to reduced sample size
  - Least desirable missing data scenario
- When MNAR occurs, research subjects affect variables
- Example: Subjects don't disclose accurate information for fear or shame; forego providing data altogether
  


## Machine Learning to Resolve Missing Data 
- Alternate methods to alleviate missing data include:
  - Support Vector Machine (SVM)
      - Creates hyperplane that has largest distance to data points
 
    
## Machine Learning to Resolve Missing Data, continued    
   - K Nearest Neighbor (KNN)
    - Measures distances for each missing value, pulls replacement value from smallest distances
    - Pro:
       - Performs well with large amount of missing data
       - Converges faster than other methods (when using iterative versions)
    - Con: high computation time
    
    

## Types of Missing Data Solutions 
- Missing Imputation (MI)
- Complete Case Analysis, aka Listwise Analysis
- Treatment and Reporting of Missing Data in Observational Studies (TARMOS)
- Full-Information Maximum Likelihood
- Multivariate Imputation by Chained Equation (MICE)
- Single Center Imputation from Multiple Chained Equation (SICE)



## Multiple Imputation (MI)
  - Most common method
  - Results similar to using complete datasets 
    - Resolves issue of too small or too large standard errors
     -   Large standard error (SE) - results acquired lacked precision
     -   Small standard error (SE) - results acquired with overestimation of precision


## Complete Case Analysis
- aka Listwise Analysis
  - Not appropriate for all use cases
  - Purpose: Eliminate missing data records altogether; analyze only remaining whole data records [@k13]
  
  
  
## Treatment and Reporting of Missing Data in Observational Studies (TARMOS)
  - Alternative to MI
  - Purpose: Provide transparency, limit redundancies in substituted data
    - Requires auxiliary variables to gather information from incomplete observations [-@ltclbghc21]
    
    
## Full-Information Maximum Likelihood   
- Uses planned missing data designs:
    - Multiform questionnaire protocol
    - Two-method planned missing design
    - Wave-missing longitudinal design 
  - Purpose: Better understand how to overcome missing data issues [@ljlmw14]


## Multivariate Imputation by Chained Equation (MICE)
- Assumes data is Missing at Random (MAR)
  - Purpose: Consider uncertainty of missing data by using multiple imputation methods to provide better results


## Figure 1: Multiple Imputation Process using 5 sets

![Figure 1: Multiple Imputation Process using 5 sets](MultiImputation.gif)  


## Single Center Imputation from Multiple Chained Equation (SICE)
- Alternative to MICE
- Improves upon MICE by creating hybrid of single and multiple imputation techniques
- Uses the respective SICE variant (categorical or numeric)
  - Missing data values corrected using thorough approach to find more accurate value to use
  - Uses predicted values imputed from MI approach 
    - Computes mean or mode for imputed values (depending on data type)
    - Replaces original imputed value with respective central measure (mean or mode)
- Has lowest computation time all all missing data solutions
  - Purpose: Replace predicted imputed values computed using MI with central measure computed using SICE [-@kh20]



## Methods

-   Dataset: 21 specific chronic illnesses of Medicare beneficiaries, from U.S. Department of Health & Human Services [@CMMS].

-   Goal: Fill in missing data (2244 missing entries, about 5%)

-   Approach: Imputation using 3 approaches: Predictive Means Matching, Classification and Regression Trees, Lasso Regression, and Random Forest

    -   These are all forms of Multiple Imputation: perform single imputation several times to create multiple data sets, analyze and compute the error of each set, then combine the data into a single, final data set [@ljlmw14].

## Rubin's rules for combining {.smaller}

1\) Select independent variables that may help impute variables with missing data

2\) Noting the chosen statistical method, estimate in each of the imputed datasets the association of interest

3\) Combine using Rubin's rules the association measures from each imputed dataset. To combine, we use the following equations

$$
W=\frac{\sum({SE_t}^2)}{m} \tag{1}
$$

$$
B=\frac{\sum(\hat{\theta_t}-\bar{\theta})^2}{m-1} \tag{2}
$$

$$
SE=\sqrt{W+B+\frac{B}{m}} \tag{3} 
$$

## Predictive Means Matching (PMM) {.smaller}

-   hot deck method

-   easy to use

-   handles any data type

-   Realistic

-   Hard to use for small data sets or ones with large FMI

For each missing variable, $x$ , we impute using parameters $\alpha$ and covariates $z$ . We use $h$ as a subscript for data containing $x$ and $j$ for data without. PMM choses a random donor from $x_h$ such that the distance defined below is minimized.

$$
\delta_{hj}=\alpha^{mis}z_j-\alpha^{obs}z_h \tag{4}
$$

## Classification and Regression Trees(CART)

-   Machine Learning

-   Robust

-   Flexible

-   Straightforward(in R)

    -   Automates variable selection, missing values, outliers, variable interaction, and nonlinear relationships

-   In practice works similarly to PMM with tree instead of regression

## Random Forest(Miss Forest)

-   Subset of CART

```{=html}
<!-- -->
```
-   Uses CART and Rubin's rules automatically to yield a single dataset

-   Does not account for uncertainty and increases P values

-   Usually similar to average of all predictions from the CART models

-   Better predictions and accuracy than a single CART model

## Lasso Regression {.smaller}

-   Minimizes regression coefficient (good for high multicollinearity)

-   Best for high dimension datasets

-   Preserves relationships between variables best

-   May add bias

-   Removes some predictor variables(easier to use)

Coefficients $\hat{\beta_0}$ and $\hat{\beta}^{lasso}$ are estimated by

$$(\hat{\beta_0}, \hat{\beta}^{lasso}) =argmin[\sum(Y_i-(\beta_0+\beta X^T_i))^2+\lambda \sum |\beta_j|] \tag5$$

where Y and X are the outcome and predictors respectively. λ is a non-negative tuning parameter that controls the amount of shrinkage, with increased shrinkage for higher λ values [@MZPRG14].

## Analysis and Results

Prior to implementing the MICE method, we ran a missing map to get a count of which rows and columns contain missing data and how many data points are missing. As the following table and figures will show most of the columns contain their complete data set. We do, however, have three columns -- Provisional Income, Total Medicare Standardized Payment, and Total Medicare Payment. Provisional Income has 408 missing entries, and both payment columns have missing values in 918 rows each. In our data set we have 2244 missing data points total between these columns.

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages

library(ggplot2)

library(dplyr)

library(mice)

library(missForest)

library(VIM)

library(ggmice)

library(xlsx)

library(readxl)

library(knitr)


```

```{r, warning=FALSE, echo=T, message=FALSE}
## reading data file from github

Chronic_Conditions <- read_excel("Chronic_Conditions.xlsx")

#Calculate pattern of missing data

Chronic_Conditions <- Chronic_Conditions %>%

select( PrvInc , Stdzd_Pymt_PC, Pymt_PC)

## Display table of missing entries per variable(complete columns were omitted)

plot_pattern(Chronic_Conditions)

```

Figure 3: Missing Map

```{r}
#Histogram of that same missingness data

aggr_plot <- aggr(Chronic_Conditions, col=c('navyblue','red'),

numbers=TRUE, sortVars=FALSE, labels=names(data), cex.axis=.7, gap=3,

ylab=c("Histogram of missing data","Pattern"))

```

Figure 4: Histogram

We can see that over 75% of the data has been recorded but between those columns we are missing roughly 25% of our data which we will utilize the MICE package to impute.

Following the Missing Map, we ran the MICE package in R.

Results:

```{r, warning=FALSE, echo=T, message=FALSE}
# Imputes missing data using the three selected methods 
mice_imputed <- data.frame(
  original = Chronic_Conditions$PrvInc,
 imputed_pmm = complete(mice(Chronic_Conditions, method = "pmm", printFlag = FALSE))$PrvInc,
  imputed_cart = complete(mice(Chronic_Conditions, method = "cart", printFlag = FALSE))$PrvInc,
  imputed_lasso = complete(mice(Chronic_Conditions, method = "lasso.norm", printFlag = FALSE))$PrvInc)



#head(mice_imputed)
#Imputation using miss Forest 
Chronic_Conditions.mis <- prodNA(Chronic_Conditions, noNA = 0.1)



```

After running the MICE package, we ran a MICE plot to gain a better visual of our missing data.

```{r, warning=FALSE, echo=T, message=FALSE}
# Plots missing data percentages per variable

aggr(Chronic_Conditions.mis, col=c('navyblue', 'yellow' ),

numbers=TRUE, sortVars=FALSE,

labels=names(Chronic_Conditions.mis), cex.axis=.7,

gap=3, ylab=c("Missing Data", "Pattern"))

```

Figure 5: Missing data percentages per variable

```{r}
# New imputation using PMM

imputed_Data <- mice(Chronic_Conditions.mis, m=5, maxit=50, method = 'pmm', seed=500, printFlag = FALSE)

#Outputs Tabular and graphical representation of that imputation

md.pattern(Chronic_Conditions.mis)
```

Additionally, we ran the miss Forest package followed by a summary to gain a better quantitative view. Below are the code, results, and figure.

```{r, warning=FALSE, echo=T, message=FALSE}
#Missingness data for miss Forest

plot_pattern(Chronic_Conditions.mis)
```

Figure 6: Table of missing values present in each variable in dataset

```{r, warning=FALSE, echo=T, message=FALSE}
#Summarizes the POOLED data after miss Forest imputation

summary(Chronic_Conditions.mis)
```

```{r, warning=FALSE, echo=T, message=FALSE}
# Density plot to compare to original data

densityplot(imputed_Data)
```

Figure 7: Density Plot with original data distribution in Blue and each red line is a different imputation

## Conclusion

Missing data is a common finding when analyzing datasets, yet can be easily addressed. Unlike other data analysis cleaning methods which create biased results by removing data or imputing an average, Multiple imputation is the act of completing data sets where there are missing data points. MI's purpose is to utilize multiple analyses for what the missing data value could be, then combine analyses results in order to have a less biased and more accurate outcome. Multivariate Imputation by Chained Equations, or MICE, in R is a highly effective way to impute missing data into data sets. One key benefit of using MI, or more specifically MICE, is to reduce uncertainty by using multiple possible outcomes for each data point. Within the MICE package are PMM, Lasso and Cart methods if imputation. Each method will produce results based off of a different type of analysis which will include both a combination of the different analysis types and a combination of each imputation.

From our data set we can see with our final Density Plot that not only does one or some of the imputed data results follow the trendline closely, but all do. We can determine that the imputed data, when this close to the trendline, will be fairly accurate.

## References
